# -*- coding: utf-8 -*-
"""Copy of Adabosttech.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o_twrCvpjEBkafZzO2A_nGLjc1pUrqii
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import re
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
nltk.download('stopwords')
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from sklearn.metrics import roc_curve,auc
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import make_scorer,accuracy_score

df = pd.read_csv('/content/drive/MyDrive/Dataset_5971.csv')

print(df.columns)

print(df)

# Drop missing values and lowercase labels
df.dropna(subset=['LABEL', 'TEXT'], inplace=True)
df['LABEL'] = df['LABEL'].str.lower()

# Filter for 'ham' and 'smishing' only
df = df[df['LABEL'].isin(['ham', 'smishing'])]

# Plot original class distribution
label_counts = df['LABEL'].value_counts()
plt.figure(figsize=(6, 4))
label_counts.plot(kind='bar', color=['skyblue', 'orange'])
plt.title('Original Class Distribution')
plt.xlabel('Class')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

tfidf = TfidfVectorizer(max_features=3000)
X_tfidf = tfidf.fit_transform(df["TEXT"])
df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())

# Encode labels
le = LabelEncoder()
labels = le.fit_transform(df["LABEL"])  # Must be full column, not just unique values

df_tfidf["LABEL"] = labels

label_map = {'ham': 0, 'smishing': 1}
labels = df["LABEL"].map(label_map)  # make sure 'label' column exists

# Extract raw features
df_features = pd.DataFrame()
df_features["msg_length"] = df["TEXT"].apply(len)
df_features["num_digits"] = df["TEXT"].str.count(r'\d')
df_features["num_specials"] = df["TEXT"].str.count(r'\W')
df_features["num_words"] = df["TEXT"].apply(lambda x: len(x.split()))
df_features["has_url"] = df["URL"].notnull().astype(int)
df_features["has_email"] = df["EMAIL"].notnull().astype(int)
df_features["has_phone"] = df["PHONE"].notnull().astype(int)
#df_features["LABEL"] = df["LABEL_encoded"]

# Drop features with zero variance
df_filtered = df_features.loc[:, df_features.nunique() > 1]

# Compute correlation matrix
corr_matrix = df_filtered.corr(method='pearson')

# Plot heatmap
plt.figure(figsize=(13, 11))
sns.heatmap(
    corr_matrix,
    annot=True, fmt=".3f", cmap='coolwarm', cbar=True
)
plt.title("Raw Text Features and Label - Pearson Correlation Heatmap (Before Preprocessing)")
plt.tight_layout()
plt.show()

# TF-IDF with clean token pattern
vectorizer = TfidfVectorizer(
    max_features=3000,
    token_pattern=r'\b[a-zA-Z]{2,}\b',
    stop_words='english'
)
X = vectorizer.fit_transform(df["TEXT"])  # make sure this is the column with actual messages

# Convert to DataFrame
X_dense = X.toarray()
feature_names = vectorizer.get_feature_names_out()
df_tfidf = pd.DataFrame(X_dense, columns=feature_names)

# Add label column
df_tfidf["label"] = labels

# Correlation matrix
corr_matrix = df_tfidf.corr(method='pearson')

# Top 20 correlated features + label
top_features = corr_matrix["label"].abs().sort_values(ascending=False).head(21).index

# Heatmap
plt.figure(figsize=(13, 11))
sns.heatmap(
    corr_matrix.loc[top_features, top_features],
    annot=True, fmt=".3f", cmap='coolwarm', cbar=True
)
plt.title("TF-IDF Features and Label - Pearson Correlation Heatmap")
plt.tight_layout()
plt.show()

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+", "url", text)  # Replace links
    text = re.sub(r'\d+', 'number', text)          # Replace numbers
    text = re.sub(r'[^\w\s]', '', text)            # Remove punctuation
    text = re.sub(r'\s+', ' ', text).strip()
    words = text.split()
    words = [word for word in words if word not in stopwords.words('english')]
    return ' '.join(words)

df['clean_text'] = df['TEXT'].apply(preprocess_text)

# Label encoding
le = LabelEncoder()
df['LABEL_encoded'] = le.fit_transform(df['LABEL'])

vectorizer = TfidfVectorizer(max_features=5000)  # Increased features for better coverage
X = vectorizer.fit_transform(df['clean_text'])
y = df['LABEL_encoded']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

svm_clf = SVC(kernel='linear', probability=True)
svm_clf.fit(X_train, y_train)
clf = svm_clf

from sklearn.model_selection import cross_val_score
#lr = LogisticRegression()
scores = cross_val_score(lr, X, y, cv=5, scoring='accuracy')

ros = RandomOverSampler(random_state=42)
X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)

plt.figure()
plt.pie([sum(y_train_resampled == 0), sum(y_train_resampled == 1)],
        labels=le.inverse_transform([0, 1]),
        autopct='%1.1f%%', colors=['green', 'blue'], startangle=90)
plt.title("Class Distribution After Oversampling")
plt.axis('equal')
plt.show()

from sklearn.linear_model import LogisticRegression

# Train Logistic Regression (Linear Model) on Resampled Data
lr = LogisticRegression(max_iter=1000, random_state=42)
lr.fit(X_train_resampled, y_train_resampled)
y_pred_lr = lr.predict(X_test)
acc_lr = accuracy_score(y_test, y_pred_lr)

# Train AdaBoost (Non-Linear Model) on Resampled Data
ada = AdaBoostClassifier(n_estimators=100, random_state=42)
ada.fit(X_train_resampled, y_train_resampled)
y_pred_ada = ada.predict(X_test)
acc_ada = accuracy_score(y_test, y_pred_ada)

print(f"Logistic Regression Accuracy (Linear): {acc_lr:.4f}")
print(f"AdaBoost Accuracy (Non-linear):       {acc_ada:.4f}")

# Conclusion
if acc_ada > acc_lr + 0.03:
    print("\n The data likely exhibits **non-linear** patterns.")
elif acc_lr >= acc_ada:
    print("\n The data appears more **linear**.")
else:
    print("\n Results are close ‚Äî the dataset may contain both linear and non-linear characteristics.")

lr = LogisticRegression(max_iter=1000)
rf_clf = RandomForestClassifier(random_state=42)
ada = AdaBoostClassifier()
xgb = XGBClassifier(eval_metric='logloss', random_state=42)

xgb.fit(X_train_resampled, y_train_resampled)
y_pred = xgb.predict(X_test)

lr.fit(X_train_resampled, y_train_resampled)
svm_clf.fit(X_train_resampled, y_train_resampled)
rf_clf.fit(X_train_resampled, y_train_resampled)
ada.fit(X_train_resampled, y_train_resampled)
xgb.fit(X_train_resampled, y_train_resampled)

X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)

scorers = {
    'accuracy': make_scorer(accuracy_score)
}
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "SVM": SVC(kernel='linear'),
    "Random Forest": RandomForestClassifier(random_state=42),
    "AdaBoost": AdaBoostClassifier(),
    "XGBoost": XGBClassifier(eval_metric='logloss')
}

# Cross-validation object (stratified to preserve label distribution)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

xgb = XGBClassifier(eval_metric='logloss')
xgb.fit(X_train, y_train)

# Evaluate each model
for name, model in models.items():
    print(f"\nüîç Model: {name}")
    for metric_name, scorer in scorers.items():
        scores = cross_val_score(model, X, y, cv=cv, scoring=scorer)
        print(f"‚úÖ {metric_name.capitalize():<10}: {scores.mean():.4f}")

models = {
    "Logistic Regression": lr,
    "SVM": svm_clf,
    "Random Forest": rf_clf,
    "AdaBoost": ada,
    "XGBoost": xgb
}

for name, model in models.items():
    print(f"\nüîç Classification Report - {name}")
    y_pred = model.predict(X_test)
    print(classification_report(y_test, y_pred, target_names=le.inverse_transform([0, 1])))

y_pred = xgb.predict(X_test)
cm = confusion_matrix(y_test, y_pred, labels=xgb.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.inverse_transform([0, 1]))
disp.plot(cmap='Oranges')
plt.title("Confusion Matrix - XGBoost")
plt.xlabel("Predicted Label")
plt.ylabel("Actual label")
plt.show()

y_pred = lr.predict(X_test)
cm = confusion_matrix(y_test, y_pred, labels=lr.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.inverse_transform([0, 1]))
disp.plot(cmap='Blues')
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted Label")
plt.ylabel("Actual label")
plt.show()

y_pred = ada.predict(X_test)
cm = confusion_matrix(y_test, y_pred, labels=ada.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.inverse_transform([0, 1]))
disp.plot(cmap='Reds')
plt.title("Confusion Matrix - Adaboost")
plt.xlabel("Predicted Label")
plt.ylabel("Actual label")
plt.show()

y_pred = svm_clf.predict(X_test)
labels = svm_clf.classes_
cm = confusion_matrix(y_test, y_pred, labels=labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.inverse_transform([0, 1]))
disp.plot(cmap='Greens')
plt.title("Confusion Matrix - SVM")
plt.xlabel("Predicted Label")
plt.ylabel("Actual label")
plt.show()

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
labels = rf.classes_
cm = confusion_matrix(y_test, y_pred, labels=labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.inverse_transform([0, 1]))
disp.plot(cmap='YlGnBu')

plt.title("Confusion Matrix - Random Forest")
plt.xlabel("Predicted Label")
plt.ylabel("Actual Label")
plt.show()

sample_messages = [
    "Hey, just wanted to remind you about the meeting at 3 PM today.",
    "Can you send me the notes from yesterday‚Äôs lecture?",
    "URGENT: Your account has been locked. Click here to unlock: http://scamlink.com",
    "Bank Alert: Unusual login detected. Verify identity immediately: www.fakebank.com",
]

preprocessed_samples = [preprocess_text(msg) for msg in sample_messages]
sample_vectorized = vectorizer.transform(preprocessed_samples)
sample_predictions = clf.predict(sample_vectorized)
sample_proba = clf.predict_proba(sample_vectorized)

print("\nüß™ Sample Message Predictions:\n" + "-"*40)
for msg, pred, proba in zip(sample_messages, sample_predictions, sample_proba):
    label = le.inverse_transform([pred])[0]
    print(f"üìù Message: {msg}")
    print(f"‚û°Ô∏è Prediction: {label.capitalize()}\n")

messages = [
    "Your OTP is 8945. Do not share it with anyone.",
    "Let's catch up later at 5 PM near the cafe!",
    "You've won a $1000 Walmart gift card. Claim now.",
]

preprocessed_samples = [preprocess_text(msg) for msg in messages]
sample_vectorized = vectorizer.transform(preprocessed_samples)
sample_predictions = clf.predict(sample_vectorized)

for msg, pred in zip(messages, sample_predictions):
    label = le.inverse_transform([pred])[0]
    print(f"üìù Message: {msg}\n‚û°Ô∏è Prediction: {label.capitalize()}\n")

models = {
    "Logistic Regression": lr,
    "SVM": svm_clf,
    "Random Forest": rf_clf,
    "AdaBoost": ada,
    "XGBoost": xgb
}

plt.figure(figsize=(10, 6))
for name, model in models.items():
    # Get predicted probabilities (for positive class: index 1)
    if hasattr(model, "predict_proba"):
        y_scores = model.predict_proba(X_test)[:, 1]
    else:
        # For SVM if no predict_proba, use decision_function
        y_scores = model.decision_function(X_test)

    # ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_scores)
    roc_auc = auc(fpr, tpr)

    # Accuracy
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)

    # Plot
    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f}, Acc = {acc:.2f})")

# Plot settings
plt.plot([0, 1], [0, 1], 'k--', label="Random Classifier")
plt.title("ROC Curve Comparison")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

import random
import re

def generate_variant_text(msg):
    msg = re.sub(r'\bclick\b', 'cl!ck', msg, flags=re.IGNORECASE)
    msg = re.sub(r'\bverify\b', 'v3rify', msg, flags=re.IGNORECASE)
    msg = re.sub(r'\baccount\b', 'acc0unt', msg, flags=re.IGNORECASE)
    msg = re.sub(r'\bfree\b', 'fr33', msg, flags=re.IGNORECASE)
    words = re.findall(r'\b\w+\b', msg)
    if words:
        selected = random.choice(words)
        msg = msg.replace(selected, ' '.join(selected), 1)
    if random.random() > 0.5:
        msg += " Let's meet tomorrow."
    return msg

print("\nüß™ EVA Adversarial Smishing Test:\n" + "-"*50)

# Pick 5 real smishing messages from your dataset
sample_smishing = df[df['LABEL'] == 'smishing']['TEXT'].sample(5, random_state=42).tolist()

# Generate adversarial variants
adversarial_samples = [generate_variant_text(msg) for msg in sample_smishing]

# Preprocess & vectorize
original_pre = [preprocess_text(msg) for msg in sample_smishing]
variant_pre = [preprocess_text(msg) for msg in adversarial_samples]

X_original = vectorizer.transform(original_pre)
X_variant = vectorizer.transform(variant_pre)

# Predict with the model you're currently using (e.g., clf = svm_clf or xgb)
pred_orig = clf.predict(X_original)
pred_var = clf.predict(X_variant)

# Display results
for orig, var, p1, p2 in zip(sample_smishing, adversarial_samples, pred_orig, pred_var):
    print("üîí Original:", orig)
    print("‚û°Ô∏è Prediction:", le.inverse_transform([p1])[0])
    print("üé≠ Variant :", var)
    print("‚û°Ô∏è Prediction:", le.inverse_transform([p2])[0])
    print("-" * 60)